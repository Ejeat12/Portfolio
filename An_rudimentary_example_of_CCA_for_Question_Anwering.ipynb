{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQsWbSAphNfyLAUKs0ncNl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ejeat12/Portfolio/blob/main/An_rudimentary_example_of_CCA_for_Question_Anwering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this project I will demonstrate the effectiveness of the Canonical Correlation Analysis(CCA) algorithm for a question-answering task where a text corpus of arbirtary responses are collected and then sampled by the model in order to answer the desired prompt. For my details of this proposed algorithm, feel free to check out the corresponding blog post here: https://intro-to-deep-learning.blogspot.com/2023/05/youre-all-that-i-been-looking-for.html "
      ],
      "metadata": {
        "id": "q3Z1WA_K9PBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.cross_decomposition import CCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Preprocess the text data\n",
        "corpus = [\n",
        "    \"I like pie\",\n",
        "    \"I go by james\",\n",
        "    \"Eric said hi to marcus.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "preprocessed_corpus = []\n",
        "for doc in corpus:\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(doc.lower())\n",
        "    # Remove stop words\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Append preprocessed document to the corpus\n",
        "    preprocessed_corpus.append(\" \".join(filtered_tokens))\n",
        "\n",
        "# Step 2: Transform preprocessed text data into vector representations\n",
        "vectorizer = TfidfVectorizer()\n",
        "text_vectors = vectorizer.fit_transform(preprocessed_corpus).toarray()\n",
        "\n",
        "# Step 3: Apply CCA and analyze correlations between text vectors\n",
        "dummy_view = np.random.randn(len(corpus), 10)  # Create a dummy view for CCA\n",
        "\n",
        "cca = CCA(n_components=1)\n",
        "cca.fit(text_vectors, dummy_view)\n",
        "text_vectors_cca = cca.transform(text_vectors)\n",
        "\n",
        "# Step 4: Perform similarity search using cosine similarity\n",
        "query = \"What is your name?\"\n",
        "preprocessed_query = \" \".join([token for token in word_tokenize(query.lower()) if token not in stop_words])\n",
        "query_vector = vectorizer.transform([preprocessed_query]).toarray()\n",
        "query_vector_cca = cca.transform(query_vector)\n",
        "\n",
        "similarities = cosine_similarity(text_vectors_cca, query_vector_cca.reshape(1, -1))\n",
        "most_similar_index = similarities.argmax()\n",
        "\n",
        "print(\"Relevant answer:\", corpus[most_similar_index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu_Yx5pY-8qW",
        "outputId": "a7c9e564-8167-48cf-f9ba-99981dc33249"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant answer: I go by james\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# As we can see the model correctly assigns the relevant answer to the prompt of \"What is your name?\" with \"I go by James\". It should be worth noting that computational cost for this procedure likely will grow exponetially as more responses are collected, within the corpus. Therefore in a production level environment, I would propose the approximate-nearest-neighbor algorithm for efficiently searching through responses in a batch like computation, opposed to all at once. For more details click here:https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6 "
      ],
      "metadata": {
        "id": "fx-n-Duu_jsS"
      }
    }
  ]
}